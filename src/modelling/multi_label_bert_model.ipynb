{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6885e3d0-2599-4d78-add2-77089131d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.annotation.doccano import get_latest_annotated_data\n",
    "from src.data.preprocessing import DataPreprocessorHelland\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset, DatasetDict\n",
    "import ray.data\n",
    "from ray.data.preprocessors import BatchMapper\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from src.slack_alert.sofus_alert import sofus_alert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ddcd1a-b00e-4267-adf0-d7ea1dc99c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>premier league valgte å offentliggjøre tre fly...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>romelu lukaku har uttalt at han er misfornøyd ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>– la oss være ærlige. jeg liker det ikke, for ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manchester united har fått mye kritikk etter å...</td>\n",
       "      <td>[0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>her er torsdagens oddstips!</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>marko gruljic fra red star belgrade 6. januar ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005</th>\n",
       "      <td>tv 2s fotballkommentator øyvind alsaker mener ...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>de fire beste i premier league får spille i ch...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007</th>\n",
       "      <td>– virgil van dijk, alisson becker og mohamed s...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>1. januar åpner overgangsvinduet, og storklubb...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2009 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     premier league valgte å offentliggjøre tre fly...   \n",
       "1     romelu lukaku har uttalt at han er misfornøyd ...   \n",
       "2     – la oss være ærlige. jeg liker det ikke, for ...   \n",
       "3     manchester united har fått mye kritikk etter å...   \n",
       "4                           her er torsdagens oddstips!   \n",
       "...                                                 ...   \n",
       "2004  marko gruljic fra red star belgrade 6. januar ...   \n",
       "2005  tv 2s fotballkommentator øyvind alsaker mener ...   \n",
       "2006  de fire beste i premier league får spille i ch...   \n",
       "2007  – virgil van dijk, alisson becker og mohamed s...   \n",
       "2008  1. januar åpner overgangsvinduet, og storklubb...   \n",
       "\n",
       "                                                  label  \n",
       "0     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "1     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2     [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "3     [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "4     [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
       "...                                                 ...  \n",
       "2004  [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2005  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2006  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2007  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "2008  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
       "\n",
       "[2009 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_latest_annotated_data().loc[:, ['text', 'label']].dropna().reset_index()\n",
    "data = data.loc[:, ['text', 'label']]\n",
    "data_preprocessor = DataPreprocessorHelland(data['text'])\n",
    "data_preprocessor.make_lower_cased()\n",
    "data_preprocessor.remove_extra_spaces()\n",
    "data['text'] = data_preprocessor.text_series\n",
    "one_hot = MultiLabelBinarizer()\n",
    "one_hot_label = one_hot.fit_transform(data['label'])\n",
    "one_hot_label = [list(map(float, x)) for x in one_hot_label]\n",
    "data['label'] = pd.Series(list(one_hot_label))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00352e34-e2e2-492c-aa1d-d4c2b6a8f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split, validation_split = train_test_split(data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf7a91b1-45cd-40e8-b4e2-f0adb40c0a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 1601\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 401\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_pandas(train_split)\n",
    "train_dataset = train_dataset.remove_columns(['__index_level_0__'])\n",
    "validation_dataset = Dataset.from_pandas(validation_split)\n",
    "validation_dataset = validation_dataset.remove_columns(['__index_level_0__'])\n",
    "split_dict = {'train': train_dataset, 'validation': validation_dataset}\n",
    "datasets = DatasetDict(split_dict)\n",
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8d21f0-fa5c-471e-bf9a-4ca2c7e43000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-28 14:46:59,485\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset(num_blocks=1, num_rows=1601, schema={text: string, label: list<item: double>}),\n",
       " 'validation': Dataset(num_blocks=1, num_rows=401, schema={text: string, label: list<item: double>})}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray_datasets = ray.data.from_huggingface(datasets)\n",
    "ray_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83d6190f-0302-4c5e-a648-538707c91afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'NbAiLab/nb-bert-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True, model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e2aaee-1c84-4106-b553-ad479ee78a88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485448c4d305492f81c02ad75e01227b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a676e9fcf743ba98a86144a03bf7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1601\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 401\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "encoded_dataset = datasets.map(preprocess_function, batched=True)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90da029e-83bd-4533-9bae-ccb70038a7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_name = \"accuracy\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "task = 'multi_label_classification'\n",
    "batch_size = 16\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fff81ad5-8ca1-433c-8332-430e843834eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
    "from transformers import EvalPrediction\n",
    "import torch\n",
    "    \n",
    "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
    "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
    "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
    "    sigmoid = torch.nn.Sigmoid()\n",
    "    probs = sigmoid(torch.Tensor(predictions))\n",
    "    # next, use threshold to turn them into integer predictions\n",
    "    y_pred = np.zeros(probs.shape)\n",
    "    y_pred[np.where(probs >= threshold)] = 1\n",
    "    # finally, compute metrics\n",
    "    y_true = labels\n",
    "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    # return as dictionary\n",
    "    metrics = {'f1': f1_micro_average,\n",
    "               'roc_auc': roc_auc,\n",
    "               'accuracy': accuracy}\n",
    "    return metrics\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, \n",
    "            tuple) else p.predictions\n",
    "    result = multi_label_metrics(\n",
    "        predictions=preds, \n",
    "        labels=p.label_ids)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4f3ede-d41b-44d9-b57f-a33950570cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at NbAiLab/nb-bert-large were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at NbAiLab/nb-bert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_checkpoint, num_labels=22, problem_type=\"multi_label_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a07a7794-bae1-49f0-a968-86d397d5a82f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eirik\\sports-article-classification\\src\\modelling\\nb-bert-large-finetuned-multi_label_classification is already a clone of https://huggingface.co/hellund/nb-bert-large-finetuned-multi_label_classification. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "validation_key = \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c54338c2-e11b-4d42-8c50-89a51da51157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from nb-bert-large-finetuned-multi_label_classification/checkpoint-303.\n",
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 1601\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 505\n",
      "  Number of trainable parameters = 355109910\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 3\n",
      "  Continuing training from global step 303\n",
      "  Will skip the first 3 epochs then the first 0 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187087c503a9464faa185d4405b6929d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='505' max='505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [505/505 1:36:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Roc Auc</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.136843</td>\n",
       "      <td>0.635762</td>\n",
       "      <td>0.756853</td>\n",
       "      <td>0.426434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.135300</td>\n",
       "      <td>0.134019</td>\n",
       "      <td>0.647781</td>\n",
       "      <td>0.762611</td>\n",
       "      <td>0.428928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to nb-bert-large-finetuned-multi_label_classification\\checkpoint-404\n",
      "Configuration saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-404\\config.json\n",
      "Model weights saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-404\\pytorch_model.bin\n",
      "tokenizer config file saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-404\\tokenizer_config.json\n",
      "Special tokens file saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-404\\special_tokens_map.json\n",
      "tokenizer config file saved in nb-bert-large-finetuned-multi_label_classification\\tokenizer_config.json\n",
      "Special tokens file saved in nb-bert-large-finetuned-multi_label_classification\\special_tokens_map.json\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to nb-bert-large-finetuned-multi_label_classification\\checkpoint-505\n",
      "Configuration saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-505\\config.json\n",
      "Model weights saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-505\\pytorch_model.bin\n",
      "tokenizer config file saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-505\\tokenizer_config.json\n",
      "Special tokens file saved in nb-bert-large-finetuned-multi_label_classification\\checkpoint-505\\special_tokens_map.json\n",
      "tokenizer config file saved in nb-bert-large-finetuned-multi_label_classification\\tokenizer_config.json\n",
      "Special tokens file saved in nb-bert-large-finetuned-multi_label_classification\\special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from nb-bert-large-finetuned-multi_label_classification\\checkpoint-505 (score: 0.428927680798005).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sofus has sent an alert - Check slack!\n"
     ]
    }
   ],
   "source": [
    "trainer.train(\"nb-bert-large-finetuned-multi_label_classification/checkpoint-303\")\n",
    "sofus_alert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "516c801b-ded3-4814-beb1-19c2a71fe2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 401\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 03:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.13401861488819122,\n",
       " 'eval_f1': 0.647780925401322,\n",
       " 'eval_roc_auc': 0.7626107290024299,\n",
       " 'eval_accuracy': 0.428927680798005,\n",
       " 'eval_runtime': 215.6616,\n",
       " 'eval_samples_per_second': 1.859,\n",
       " 'eval_steps_per_second': 0.121,\n",
       " 'epoch': 5.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da08754b-d85a-4917-9ae0-af24e9fda92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '- Jeg elsker å score mål på corner. Vi var mye bedre i luften enn Brann idag'\n",
    "encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = trainer.model(**encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab2dc2a1-4bc5-43a3-a7ad-e242eb8bc1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "372d1265-17bc-4287-8fbe-064fd0a0eb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Quote',)]\n"
     ]
    }
   ],
   "source": [
    "# apply sigmoid + threshold\n",
    "sigmoid = torch.nn.Sigmoid()\n",
    "probs = sigmoid(logits.squeeze().cpu())\n",
    "predictions = np.zeros(probs.shape)\n",
    "predictions[np.where(probs >= 0.5)] = 1\n",
    "# turn predicted id's into actual label names\n",
    "predictions = np.reshape(predictions, (1, predictions.shape[0]))\n",
    "predicted_labels = one_hot.inverse_transform(predictions)\n",
    "print(predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
